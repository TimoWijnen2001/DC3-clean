{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b284a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all imports\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bf9f8c",
   "metadata": {},
   "source": [
    "### 1.1 Prepare the bleaching dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b64c41",
   "metadata": {},
   "source": [
    "Originally, the bleaching dataset only has bleached and non-bleached masks. \n",
    "The union of these two masks is saved to represent a coral mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b0dd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directories\n",
    "root_dir = Path(\"data/coral_bleaching\")\n",
    "images_dir = root_dir / \"images\"\n",
    "bleached_dir = root_dir / \"masks_bleached\"\n",
    "non_bleached_dir = root_dir / \"masks_non_bleached\"\n",
    "out_dir = root_dir / \"masks_coral\"\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Loop over all images, and create the coral masks\n",
    "created = fails = 0\n",
    "for img_path in sorted(images_dir.glob(\"*.jpg\")):\n",
    "    stem = img_path.stem\n",
    "    bleached_path = bleached_dir / f\"{stem}_bleached.png\"\n",
    "    non_bleached_path = non_bleached_dir / f\"{stem}_non_bleached.png\"\n",
    "    \n",
    "    mask_bleached = np.array(Image.open(bleached_path).convert(\"L\")).astype(np.uint8)\n",
    "    mask_non_bleached = np.array(Image.open(non_bleached_path).convert(\"L\")).astype(np.uint8)\n",
    "    mask_coral = (mask_bleached | mask_non_bleached).astype(np.uint8)\n",
    "\n",
    "    output_path = out_dir / f\"{stem}.png\"\n",
    "    Image.fromarray((mask_coral).astype(np.uint8)).save(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4d978d",
   "metadata": {},
   "source": [
    "The bleaching dataset and new coral mask is saved into a DataFrame.  \n",
    "Any images with less than 1% coral coverage are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0088606c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all datasets, and remove any coral masks that have less than 1% coverage.\n",
    "\n",
    "root_dir = Path(\"data/coral_bleaching\")\n",
    "images_dir = root_dir / \"images\"\n",
    "coral_dir = root_dir / \"masks_coral\"\n",
    "\n",
    "rows = []\n",
    "\n",
    "for img_path in sorted(images_dir.glob(\"*.jpg\")):\n",
    "    stem = img_path.stem\n",
    "  \n",
    "    paths = {\n",
    "        \"image\": img_path,\n",
    "        \"mask_coral\": coral_dir / f\"{stem}.png\",\n",
    "    }\n",
    "    \n",
    "    array = np.array(Image.open(paths[\"mask_coral\"]).convert(\"L\")).astype(np.uint8) / 255\n",
    "    coral_mask_H, coral_mask_W = array.shape\n",
    "    coral_coverage = array.sum() / (coral_mask_H * coral_mask_W)\n",
    "    if coral_coverage >= 0.01:\n",
    "        rows.append({\n",
    "        \"stem\": stem,\n",
    "        \"image_path\": str(paths[\"image\"]),\n",
    "        \"mask_coral_path\": str(paths[\"mask_coral\"]) if paths[\"mask_coral\"].exists() else None,\n",
    "        \"coral_coverage\": round(float(coral_coverage), 3),\n",
    "        \"source\": \"coral_bleaching_reefsupport\", \n",
    "    })\n",
    "\n",
    "df_clean = pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e22526",
   "metadata": {},
   "source": [
    "### 1.2 Prepare the benthic dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69233397",
   "metadata": {},
   "source": [
    "The benthic dataset has masks for hard and soft corals. We create a new mask with the union of these two to represent a general coral mask.  \n",
    "Any images with less than 1% coral coverage are removed, and the results are appended to the previous DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bce5a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = Path(\"data/benthic_datasets\")\n",
    "\n",
    "for site in sorted(os.listdir(root_dir)):\n",
    "    images_dir = root_dir / site / \"images\"\n",
    "    coral_stitched_dir = root_dir / site / \"masks_stitched\"\n",
    "    coral_dir = root_dir / site / \"masks_coral\"\n",
    "    coral_dir.mkdir(parents=True, exist_ok=True)\n",
    "    rows = []\n",
    "\n",
    "    for img_path in sorted(images_dir.glob(\"*.[jJ][pP][gG]\")):\n",
    "        stem = img_path.stem\n",
    "        \n",
    "        # Save all paths\n",
    "        paths = {\n",
    "            \"image\": img_path,\n",
    "            \"coral_stitched_dir\": coral_stitched_dir / f\"{stem}_mask.png\",\n",
    "            \"mask_coral\": coral_dir / f\"{stem}_binary.png\",\n",
    "        }\n",
    "\n",
    "        # Convert rgb array to coral-only binary array and save the new array\n",
    "        with Image.open(paths[\"coral_stitched_dir\"]) as mm:\n",
    "            rgb_array = np.array(mm.convert(\"RGB\"))\n",
    "            binary_array = (rgb_array.any(axis=-1)).astype(np.uint8) * 255\n",
    "        out_path = coral_dir / f\"{stem}_binary.png\"\n",
    "        Image.fromarray(binary_array).save(out_path)\n",
    "\n",
    "\n",
    "        # Compute the coral coverage, exclude any images with <0.01 coverage, and add the others to the dataframe\n",
    "        coral_mask_H, coral_mask_W = binary_array.shape\n",
    "        coral_coverage = (binary_array/255).sum() / (coral_mask_H * coral_mask_W)\n",
    "        if coral_coverage >= 0.01:\n",
    "            rows.append({\n",
    "            \"stem\": stem,\n",
    "            \"image_path\": str(paths[\"image\"]),\n",
    "            \"mask_coral_path\": str(paths[\"mask_coral\"]) if paths[\"mask_coral\"].exists() else None,\n",
    "            \"coral_coverage\": round(float(coral_coverage), 3),\n",
    "            \"source\": str(site), \n",
    "        })\n",
    "\n",
    "    df_clean = pd.concat([df_clean, pd.DataFrame(rows)], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c876e783",
   "metadata": {},
   "source": [
    "### 1.3 Train/test/val split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20207039",
   "metadata": {},
   "source": [
    "A train/test/val split of 70/15/15 is good practice.  \n",
    "To ensure domain robustness across reef locations, images from one source should not be in multiple splits.  \n",
    "To achieve this, the images from sources are divided among the splits.  \n",
    "The division that was used achieves a 72/13/15 split, which is quite good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c736760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the number of images per source\n",
    "for source in np.unique(df_clean[\"source\"]):\n",
    "    print(f\"{source}: {len(df_clean[df_clean[\"source\"] == source])}\")\n",
    "print()\n",
    "\n",
    "# Create a custom division of the sources over the 3 splits.\n",
    "train_sources = [\"SEAFLOWER_BOLIVAR\", \"SEAVIEW_ATL\", \"SEAVIEW_PAC_AUS\", \"UNAL_BLEACHING_TAYRONA\", \"coral_bleaching_reefsupport\"]\n",
    "test_sources = [\"SEAFLOWER_COURTOWN\", \"SEAVIEW_PAC_USA\"]\n",
    "val_sources = [\"SEAVIEW_IDN_PHL\", \"TETES_PROVIDENCIA\"]\n",
    "\n",
    "df_clean[\"split\"] = df_clean[\"source\"].apply(\n",
    "    lambda s: \"train\" if s in train_sources else\n",
    "              \"test\" if s in test_sources else\n",
    "              \"val\" if s in val_sources else None\n",
    ")\n",
    "\n",
    "# Save the csv file\n",
    "Path(\"csv_folder/\").mkdir(parents=True, exist_ok=True)\n",
    "df_clean.to_csv(\"csv_folder/df_clean.csv\")\n",
    "\n",
    "# Show train/test/val ratios\n",
    "split_counts = df_clean[\"split\"].value_counts().sort_index()\n",
    "split_perc = (split_counts / len(df_clean) * 100).round(1)\n",
    "\n",
    "for split in [\"train\", \"val\", \"test\"]:\n",
    "    count = split_counts.get(split, 0)\n",
    "    perc = split_perc.get(split, 0)\n",
    "    print(f\"  {split:<6}: {count:>4} images ({perc:>4.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bae0e3",
   "metadata": {},
   "source": [
    "### 1.4 Resize the images and masks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcf8cf7",
   "metadata": {},
   "source": [
    "After some testing, we decided to resize all images and masks to 512x512.  \n",
    "This seemed like a good balance between not affecting model accuracy and maintaining an affordable computation effort. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c53649",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "output_dir = Path(\"data/final_dataset\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "img_size = (512,512)\n",
    "rows_out = []\n",
    "\n",
    "for _, row in tqdm(df_clean.iterrows(), total=len(df_clean)):\n",
    "    # Define variables per row\n",
    "    stem = row[\"stem\"]\n",
    "    img_path = Path(row[\"image_path\"])\n",
    "    mask_path = Path(row[\"mask_coral_path\"])\n",
    "    source = row[\"source\"]\n",
    "    split = row[\"split\"]\n",
    "\n",
    "    # Define directories and paths\n",
    "    out_img_dir  = output_dir / split / \"images\"\n",
    "    out_mask_dir = output_dir / split / \"masks_coral\"\n",
    "    out_img_dir.mkdir(parents=True, exist_ok=True)\n",
    "    out_mask_dir.mkdir(parents=True, exist_ok=True)\n",
    "    out_img_path  = out_img_dir / f\"{stem}.jpg\"\n",
    "    out_mask_path = out_mask_dir / f\"{stem}.png\"\n",
    "\n",
    "    # Read image and mask\n",
    "    img = cv2.imread(str(img_path), cv2.IMREAD_COLOR)\n",
    "    mask = cv2.imread(str(mask_path), cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    # Resize image and mask to 512x512\n",
    "    img_resized  = cv2.resize(img, img_size, interpolation=cv2.INTER_CUBIC)\n",
    "    mask_resized = cv2.resize(mask, img_size, interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "    # Save resized images and masks\n",
    "    cv2.imwrite(str(out_img_path), img_resized, [cv2.IMWRITE_JPEG_QUALITY, 95])\n",
    "    cv2.imwrite(str(out_mask_path), mask_resized)\n",
    "\n",
    "    # Create new row for dataframe\n",
    "    rows_out.append({\n",
    "        \"stem\": stem,\n",
    "        \"image_path\": str(out_img_path),\n",
    "        \"mask_coral_path\": str(out_mask_path),\n",
    "        \"source\": source,\n",
    "        \"split\": split,\n",
    "    })\n",
    "\n",
    "df_resized = pd.DataFrame(rows_out)\n",
    "df_resized.to_csv(\"csv_folder/resized_df.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
